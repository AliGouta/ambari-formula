{% raw %}
		{
            "hive-env": {
                "properties": {
                    "content": "\n if [ \"$SERVICE\" = \"cli\" ]; then\n   if [ -z \"$DEBUG\" ]; then\n     export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit\"\n   else\n     export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit\"\n   fi\n fi\n\n# The heap size of the jvm stared by hive shell script can be controlled via:\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR={{hive_config_dir}}\n\n# Folder containing extra libraries required for hive compilation/execution can be controlled by:\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n  if [ -f \"${HIVE_AUX_JARS_PATH}\" ]; then    \n    export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\n  elif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n    export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar\n  fi\nelif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n  export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar\nfi      \n\nexport METASTORE_PORT={{hive_metastore_port}}",
                    "hcat_log_dir": "/data/var/log/webhcat",
                    "hcat_pid_dir": "/data/var/run/webhcat",
                    "hcat_user": "hcat",
                    "hive_ambari_database": "MySQL",
                    "hive_ambari_host": "%HOSTGROUP::host_group_master_2%",
                    "hive_database": "New MySQL Database",
                    "hive_database_name": "hive",
                    "hive_database_type": "mysql",
                    "hive_existing_mssql_server_2_host": "",
                    "hive_existing_mssql_server_host": "",
                    "hive_existing_mysql_host": "",
                    "hive_existing_oracle_host": "",
                    "hive_existing_postgresql_host": "",
                    "hive_hostname": "%HOSTGROUP::host_group_master_2%",
                    "hive_log_dir": "/data/var/log/hive",
                    "hive_metastore_port": "9083",
                    "hive_pid_dir": "/data/var/run/hive",
                    "hive_user": "hive",
                    "webhcat_user": "hcat"
                }
            }
        },
        {
            "hive-exec-log4j": {
                "properties": {
                    "content": "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Define some default values that can be overridden by system properties\n\nhive.log.threshold=ALL\nhive.root.logger=INFO,FA\nhive.log.dir=${java.io.tmpdir}/${user.name}\nhive.query.id=hadoop\nhive.log.file=${hive.query.id}.log\n\n# Define the root logger to the system property \"hadoop.root.logger\".\nlog4j.rootLogger=${hive.root.logger}, EventCounter\n\n# Logging Threshold\nlog4j.threshhold=${hive.log.threshold}\n\n#\n# File Appender\n#\n\nlog4j.appender.FA=org.apache.log4j.FileAppender\nlog4j.appender.FA.File=${hive.log.dir}/${hive.log.file}\nlog4j.appender.FA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\n#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n# Debugging Pattern format\nlog4j.appender.FA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#custom logging levels\n#log4j.logger.xxx=DEBUG\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter\n\n\nlog4j.category.DataNucleus=ERROR,FA\nlog4j.category.Datastore=ERROR,FA\nlog4j.category.Datastore.Schema=ERROR,FA\nlog4j.category.JPOX.Datastore=ERROR,FA\nlog4j.category.JPOX.Plugin=ERROR,FA\nlog4j.category.JPOX.MetaData=ERROR,FA\nlog4j.category.JPOX.Query=ERROR,FA\nlog4j.category.JPOX.General=ERROR,FA\nlog4j.category.JPOX.Enhancer=ERROR,FA\n\n\n# Silence useless ZK logs\nlog4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,FA\nlog4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,FA"
                }
            }
        },
        {
            "hive-log4j": {
                "properties": {
                    "content": "\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Define some default values that can be overridden by system properties\nhive.log.threshold=ALL\nhive.root.logger=INFO,DRFA\nhive.log.dir=${java.io.tmpdir}/${user.name}\nhive.log.file=hive.log\n\n# Define the root logger to the system property \"hadoop.root.logger\".\nlog4j.rootLogger=${hive.root.logger}, EventCounter\n\n# Logging Threshold\nlog4j.threshold=${hive.log.threshold}\n\n#\n# Daily Rolling File Appender\n#\n# Use the PidDailyerRollingFileAppend class instead if you want to use separate log files\n# for different CLI session.\n#\n# log4j.appender.DRFA=org.apache.hadoop.hive.ql.log.PidDailyRollingFileAppender\n\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\n\nlog4j.appender.DRFA.File=${hive.log.dir}/${hive.log.file}\n\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\n\n# 30-day backup\n#log4j.appender.DRFA.MaxBackupIndex=30\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\n#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n# Debugging Pattern format\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\n\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\nlog4j.appender.console.encoding=UTF-8\n\n#custom logging levels\n#log4j.logger.xxx=DEBUG\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter\n\n\nlog4j.category.DataNucleus=ERROR,DRFA\nlog4j.category.Datastore=ERROR,DRFA\nlog4j.category.Datastore.Schema=ERROR,DRFA\nlog4j.category.JPOX.Datastore=ERROR,DRFA\nlog4j.category.JPOX.Plugin=ERROR,DRFA\nlog4j.category.JPOX.MetaData=ERROR,DRFA\nlog4j.category.JPOX.Query=ERROR,DRFA\nlog4j.category.JPOX.General=ERROR,DRFA\nlog4j.category.JPOX.Enhancer=ERROR,DRFA\n\n\n# Silence useless ZK logs\nlog4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,DRFA\nlog4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,DRFA"
                }
            }
        },
        {
            "hive-site": {
                "properties": {
                    "ambari.hive.db.schema.name": "hive",
                    "datanucleus.cache.level2.type": "none",
                    "hive.auto.convert.join": "true",
                    "hive.auto.convert.join.noconditionaltask": "true",
                    "hive.auto.convert.join.noconditionaltask.size": "238026752",
                    "hive.auto.convert.sortmerge.join": "true",
                    "hive.auto.convert.sortmerge.join.to.mapjoin": "false",
                    "hive.cbo.enable": "true",
                    "hive.cli.print.header": "false",
                    "hive.cluster.delegation.token.store.class": "org.apache.hadoop.hive.thrift.ZooKeeperTokenStore",
                    "hive.cluster.delegation.token.store.zookeeper.connectString": "%HOSTGROUP::host_group_master_3%:2181,%HOSTGROUP::host_group_master_2%:2181",
                    "hive.cluster.delegation.token.store.zookeeper.znode": "/hive/cluster/delegation",
                    "hive.compactor.abortedtxn.threshold": "1000",
                    "hive.compactor.check.interval": "300L",
                    "hive.compactor.delta.num.threshold": "10",
                    "hive.compactor.delta.pct.threshold": "0.1f",
                    "hive.compactor.initiator.on": "false",
                    "hive.compactor.worker.threads": "0",
                    "hive.compactor.worker.timeout": "86400L",
                    "hive.compute.query.using.stats": "true",
                    "hive.conf.restricted.list": "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
                    "hive.convert.join.bucket.mapjoin.tez": "false",
                    "hive.enforce.bucketing": "true",
                    "hive.enforce.sorting": "true",
                    "hive.enforce.sortmergebucketmapjoin": "true",
                    "hive.exec.compress.intermediate": "false",
                    "hive.exec.compress.output": "false",
                    "hive.exec.dynamic.partition": "true",
                    "hive.exec.dynamic.partition.mode": "nonstrict",
                    "hive.exec.failure.hooks": "org.apache.hadoop.hive.ql.hooks.ATSHook",
                    "hive.exec.max.created.files": "100000",
                    "hive.exec.max.dynamic.partitions": "5000",
                    "hive.exec.max.dynamic.partitions.pernode": "2000",
                    "hive.exec.orc.compression.strategy": "SPEED",
                    "hive.exec.orc.default.compress": "ZLIB",
                    "hive.exec.orc.default.stripe.size": "67108864",
                    "hive.exec.parallel": "false",
                    "hive.exec.parallel.thread.number": "8",
                    "hive.exec.post.hooks": "org.apache.hadoop.hive.ql.hooks.ATSHook",
                    "hive.exec.pre.hooks": "org.apache.hadoop.hive.ql.hooks.ATSHook",
                    "hive.exec.reducers.bytes.per.reducer": "67108864",
                    "hive.exec.reducers.max": "1009",
                    "hive.exec.scratchdir": "/tmp/hive",
                    "hive.exec.submit.local.task.via.child": "true",
                    "hive.exec.submitviachild": "false",
                    "hive.execution.engine": "tez",
                    "hive.fetch.task.aggr": "false",
                    "hive.fetch.task.conversion": "more",
                    "hive.fetch.task.conversion.threshold": "1073741824",
                    "hive.limit.optimize.enable": "true",
                    "hive.limit.pushdown.memory.usage": "0.04",
                    "hive.map.aggr": "true",
                    "hive.map.aggr.hash.force.flush.memory.threshold": "0.9",
                    "hive.map.aggr.hash.min.reduction": "0.5",
                    "hive.map.aggr.hash.percentmemory": "0.5",
                    "hive.mapjoin.bucket.cache.size": "10000",
                    "hive.mapjoin.optimized.hashtable": "true",
                    "hive.mapred.reduce.tasks.speculative.execution": "false",
                    "hive.merge.mapfiles": "true",
                    "hive.merge.mapredfiles": "false",
                    "hive.merge.orcfile.stripe.level": "true",
                    "hive.merge.rcfile.block.level": "true",
                    "hive.merge.size.per.task": "256000000",
                    "hive.merge.smallfiles.avgsize": "16000000",
                    "hive.merge.tezfiles": "false",
                    "hive.metastore.authorization.storage.checks": "false",
                    "hive.metastore.cache.pinobjtypes": "Table,Database,Type,FieldSchema,Order",
                    "hive.metastore.client.connect.retry.delay": "5s",
                    "hive.metastore.client.socket.timeout": "1800s",
                    "hive.metastore.connect.retries": "24",
                    "hive.metastore.execute.setugi": "true",
                    "hive.metastore.failure.retries": "24",
                    "hive.metastore.kerberos.keytab.file": "/etc/security/keytabs/hive.service.keytab",
                    "hive.metastore.kerberos.principal": "hive/_HOST@EXAMPLE.COM",
                    "hive.metastore.pre.event.listeners": "org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener",
                    "hive.metastore.sasl.enabled": "false",
                    "hive.metastore.server.max.threads": "100000",
                    "hive.metastore.uris": "thrift://%HOSTGROUP::host_group_master_2%:9083",
                    "hive.metastore.warehouse.dir": "/apps/hive/warehouse",
                    "hive.optimize.bucketmapjoin": "true",
                    "hive.optimize.bucketmapjoin.sortedmerge": "false",
                    "hive.optimize.constant.propagation": "true",
                    "hive.optimize.index.filter": "true",
                    "hive.optimize.metadataonly": "true",
                    "hive.optimize.null.scan": "true",
                    "hive.optimize.reducededuplication": "true",
                    "hive.optimize.reducededuplication.min.reducer": "4",
                    "hive.optimize.sort.dynamic.partition": "false",
                    "hive.orc.compute.splits.num.threads": "10",
                    "hive.orc.splits.include.file.footer": "false",
                    "hive.prewarm.enabled": "false",
                    "hive.prewarm.numcontainers": "10",
                    "hive.security.authenticator.manager": "org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator",
                    "hive.security.authorization.enabled": "false",
                    "hive.security.authorization.manager": "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory",
                    "hive.security.metastore.authenticator.manager": "org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
                    "hive.security.metastore.authorization.auth.reads": "true",
                    "hive.security.metastore.authorization.manager": "org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider,org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly",
                    "hive.server2.allow.user.substitution": "true",
                    "hive.server2.authentication": "NONE",
                    "hive.server2.authentication.spnego.keytab": "HTTP/_HOST@EXAMPLE.COM",
                    "hive.server2.authentication.spnego.principal": "/etc/security/keytabs/spnego.service.keytab",
                    "hive.server2.enable.doAs": "true",
                    "hive.server2.logging.operation.enabled": "true",
                    "hive.server2.logging.operation.log.location": "${system:java.io.tmpdir}/${system:user.name}/operation_logs",
                    "hive.server2.support.dynamic.service.discovery": "true",
                    "hive.server2.table.type.mapping": "CLASSIC",
                    "hive.server2.tez.default.queues": "default",
                    "hive.server2.tez.initialize.default.sessions": "false",
                    "hive.server2.tez.sessions.per.default.queue": "1",
                    "hive.server2.thrift.http.path": "cliservice",
                    "hive.server2.thrift.http.port": "10001",
                    "hive.server2.thrift.max.worker.threads": "500",
                    "hive.server2.thrift.port": "10000",
                    "hive.server2.thrift.sasl.qop": "auth",
                    "hive.server2.transport.mode": "binary",
                    "hive.server2.use.SSL": "false",
                    "hive.server2.zookeeper.namespace": "hiveserver2",
                    "hive.smbjoin.cache.rows": "10000",
                    "hive.stats.autogather": "true",
                    "hive.stats.dbclass": "fs",
                    "hive.stats.fetch.column.stats": "false",
                    "hive.stats.fetch.partition.stats": "true",
                    "hive.support.concurrency": "false",
                    "hive.tez.auto.reducer.parallelism": "false",
                    "hive.tez.container.size": "682",
                    "hive.tez.cpu.vcores": "-1",
                    "hive.tez.dynamic.partition.pruning": "true",
                    "hive.tez.dynamic.partition.pruning.max.data.size": "104857600",
                    "hive.tez.dynamic.partition.pruning.max.event.size": "1048576",
                    "hive.tez.input.format": "org.apache.hadoop.hive.ql.io.HiveInputFormat",
                    "hive.tez.java.opts": "-server -Xmx546m -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps",
                    "hive.tez.log.level": "INFO",
                    "hive.tez.max.partition.factor": "2.0",
                    "hive.tez.min.partition.factor": "0.25",
                    "hive.tez.smb.number.waves": "0.5",
                    "hive.txn.manager": "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager",
                    "hive.txn.max.open.batch": "1000",
                    "hive.txn.timeout": "300",
                    "hive.user.install.directory": "/user/",
                    "hive.vectorized.execution.enabled": "true",
                    "hive.vectorized.execution.reduce.enabled": "false",
                    "hive.vectorized.groupby.checkinterval": "4096",
                    "hive.vectorized.groupby.flush.percent": "0.1",
                    "hive.vectorized.groupby.maxentries": "100000",
                    "hive.zookeeper.client.port": "2181",
                    "hive.zookeeper.namespace": "hive_zookeeper_namespace",
                    "hive.zookeeper.quorum": "%HOSTGROUP::host_group_master_3%:2181,%HOSTGROUP::host_group_master_2%:2181",
                    "javax.jdo.option.ConnectionDriverName": "com.mysql.jdbc.Driver",
                    "javax.jdo.option.ConnectionURL": "jdbc:mysql://%HOSTGROUP::host_group_master_2%/hive?createDatabaseIfNotExist=true",
                    "javax.jdo.option.ConnectionUserName": "hive"
                }
            }
        },
        {
            "hiveserver2-site": {
                "properties": {
                    "hive.security.authenticator.manager": "org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator",
                    "hive.security.authorization.manager": "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory"
                }
            }
        },
{% endraw %}