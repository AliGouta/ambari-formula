{% raw %}
    	{
            "kafka-broker": {
                "properties": {
                    "auto.create.topics.enable": "true",
                    "controlled.shutdown.enable": "false",
                    "controlled.shutdown.max.retries": "3",
                    "controlled.shutdown.retry.backoff.ms": "5000",
                    "controller.message.queue.size": "10",
                    "controller.socket.timeout.ms": "30000",
                    "default.replication.factor": "1",
                    "fetch.purgatory.purge.interval.requests": "10000",
                    "kafka.ganglia.metrics.group": "kafka",
                    "kafka.ganglia.metrics.port": "8671",
                    "kafka.ganglia.metrics.reporter.enabled": "true",
                    "kafka.metrics.reporters": "{{kafka_metrics_reporters}}",
                    "kafka.timeline.metrics.host": "{{metric_collector_host}}",
                    "kafka.timeline.metrics.maxRowCacheSize": "10000",
                    "kafka.timeline.metrics.port": "{{metric_collector_port}}",
                    "kafka.timeline.metrics.reporter.enabled": "true",
                    "kafka.timeline.metrics.reporter.sendInterval": "5900",
                    "log.cleanup.interval.mins": "10",
                    "log.dirs": "/data/mnt/kafka-logs",
                    "log.flush.interval.messages": "10000",
                    "log.flush.interval.ms": "3000",
                    "log.flush.scheduler.interval.ms": "3000",
                    "log.index.interval.bytes": "4096",
                    "log.index.size.max.bytes": "10485760",
                    "log.retention.bytes": "-1",
                    "log.retention.hours": "168",
                    "log.roll.hours": "168",
                    "log.segment.bytes": "1073741824",
                    "message.max.bytes": "1000000",
                    "num.io.threads": "8",
                    "num.network.threads": "3",
                    "num.partitions": "1",
                    "num.replica.fetchers": "1",
                    "port": "6667",
                    "producer.purgatory.purge.interval.requests": "10000",
                    "queued.max.requests": "500",
                    "replica.fetch.max.bytes": "1048576",
                    "replica.fetch.min.bytes": "1",
                    "replica.fetch.wait.max.ms": "500",
                    "replica.high.watermark.checkpoint.interval.ms": "5000",
                    "replica.lag.max.messages": "4000",
                    "replica.lag.time.max.ms": "10000",
                    "replica.socket.receive.buffer.bytes": "65536",
                    "replica.socket.timeout.ms": "30000",
                    "socket.receive.buffer.bytes": "102400",
                    "socket.request.max.bytes": "104857600",
                    "socket.send.buffer.bytes": "102400",
                    "zookeeper.connect": "%HOSTGROUP::host_group_master_3%:2181,%HOSTGROUP::host_group_master_2%:2181",
                    "zookeeper.connection.timeout.ms": "6000",
                    "zookeeper.session.timeout.ms": "30000",
                    "zookeeper.sync.time.ms": "2000"
                }
            }
        },
        {
            "kafka-env": {
                "properties": {
                    "content": "\n#!/bin/bash\n\n# Set KAFKA specific environment variables here.\n\n# The java implementation to use.\nexport JAVA_HOME={{java64_home}}\nexport PATH=$PATH:$JAVA_HOME/bin\nexport PID_DIR={{kafka_pid_dir}}\nexport LOG_DIR={{kafka_log_dir}}\n\n# Add kafka sink to classpath and related depenencies\nif [ -e \"/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\" ]; then\n  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\n  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/lib/*\nfi",
                    "kafka_log_dir": "/data/var/log/kafka",
                    "kafka_pid_dir": "/data/var/run/kafka",
                    "kafka_user": "kafka"
                }
            }
        },
        {
            "kafka-log4j": {
                "properties": {
                    "content": "\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\nkafka.logs.dir=logs\n\nlog4j.rootLogger=INFO, stdout\n\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log\nlog4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log\nlog4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log\nlog4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log\nlog4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\nlog4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log\nlog4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n\n# Turn on all our debugging info\n#log4j.logger.kafka.producer.async.DefaultEventHandler=DEBUG, kafkaAppender\n#log4j.logger.kafka.client.ClientUtils=DEBUG, kafkaAppender\n#log4j.logger.kafka.perf=DEBUG, kafkaAppender\n#log4j.logger.kafka.perf.ProducerPerformance$ProducerThread=DEBUG, kafkaAppender\n#log4j.logger.org.I0Itec.zkclient.ZkClient=DEBUG\nlog4j.logger.kafka=INFO, kafkaAppender\nlog4j.logger.kafka.network.RequestChannel$=WARN, requestAppender\nlog4j.additivity.kafka.network.RequestChannel$=false\n\n#log4j.logger.kafka.network.Processor=TRACE, requestAppender\n#log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender\n#log4j.additivity.kafka.server.KafkaApis=false\nlog4j.logger.kafka.request.logger=WARN, requestAppender\nlog4j.additivity.kafka.request.logger=false\n\nlog4j.logger.kafka.controller=TRACE, controllerAppender\nlog4j.additivity.kafka.controller=false\n\nlog4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender\nlog4j.additivity.kafka.log.LogCleaner=false\n\nlog4j.logger.state.change.logger=TRACE, stateChangeAppender\nlog4j.additivity.state.change.logger=false"
                }
            }
        },
{% endraw %}